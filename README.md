# ‚òÅÔ∏è AWS Data Lake Architecture - ShopNow Project

![AWS](https://img.shields.io/badge/AWS-Console-232F3E?style=for-the-badge&logo=amazon-aws&logoColor=white)
![Athena](https://img.shields.io/badge/Amazon_Athena-SQL-blue?style=for-the-badge)
![Glue](https://img.shields.io/badge/AWS_Glue-ETL-blue?style=for-the-badge)
![IAM](https://img.shields.io/badge/Security-IAM_Governance-red?style=for-the-badge)

##  Pr√©sentation du Projet
Conception et d√©ploiement d'une architecture **Data Lake compl√®te sur AWS** pour une entreprise E-commerce.
L'objectif √©tait de centraliser les donn√©es transactionnelles (PostgreSQL) et de les pr√©parer pour l'analyse, tout en respectant le principe de **moindre privil√®ge**.

> **M√©thodologie :** D√©ploiement via la Console AWS (Infrastructure as a Service).

---

##  Architecture D√©ploy√©e

![Architecture Diagram](architecture_diagram.png)

Le pipeline de donn√©es suit les √©tapes suivantes :
1.  **Source :** Base de donn√©es transactionnelle **Amazon RDS (PostgreSQL)** contenant les tables `clients`, `produits`, `ventes`.
2.  **Stockage (Data Lake) :**
    * `s3://shopnow-data/raw/` : Stockage des extractions brutes (CSV).
    * `s3://shopnow-data/analytics/` : Donn√©es nettoy√©es et optimis√©es via Glue.
3.  **ETL & Catalogue :** Utilisation d'**AWS Glue** (Crawlers & Jobs) pour cataloguer les donn√©es et transformer les types (casting, nettoyage).
4.  **Analyse :** Validation et exploration des donn√©es via **Amazon Athena** (SQL Serverless).

###  Zoom sur le Pipeline ETL (AWS Glue Studio)
J'ai con√ßu un workflow visuel pour joindre les tables `Ventes`, `Clients` et `Produits` et nettoyer les donn√©es avant l'export S3 :

![Glue Job Workflow](glue_job.png)

---

##  S√©curit√© & Gouvernance (IAM)

Point central du projet : la mise en place d'une politique de s√©curit√© stricte (RBAC - Role Based Access Control).
J'ai configur√© **3 r√¥les IAM distincts** pour cloisonner les acc√®s :

### 1. R√¥le `Data Engineer`
* **Responsabilit√© :** Maintenance du pipeline et transformation.
* **Permissions :**
    * ‚úÖ Lecture/√âcriture sur tous les Buckets S3 (`raw` et `analytics`).
    * ‚úÖ Acc√®s complet √† AWS Glue (Cr√©ation de Jobs/Crawlers).
    * ‚úÖ Acc√®s √† Athena.
    * ‚ö†Ô∏è Lecture seule sur RDS (Protection de la prod).

### 2. R√¥le `Data Analyst`
* **Responsabilit√© :** Cr√©ation de rapports et requ√™tes SQL.
* **Permissions :**
    * ‚úÖ Acc√®s √† Amazon Athena pour requ√™ter le Data Lake.
    * ‚ö†Ô∏è Lecture seule stricte sur S3 (`analytics/` uniquement).
    * ‚ùå **Interdiction** d'acc√®s √† la zone `raw/` ou aux jobs Glue.

---

##  Validation Technique & R√©sultats

Le pipeline ETL est fonctionnel et les donn√©es sont pr√™tes √† √™tre consomm√©es.
Validation effectu√©e via des requ√™tes SQL complexes dans **Amazon Athena** :

![Athena SQL Results](athena_sql_results.png)

*Exemple de requ√™te analytique valid√©e (CA par cat√©gorie) :*

```sql
SELECT 
    p.categorie, 
    SUM(v.montant) as chiffre_affaires_total
FROM "shopnow_db"."ventes" v
JOIN "shopnow_db"."produits" p ON v.id_produit = p.id_produit
GROUP BY p.categorie
ORDER BY chiffre_affaires_total DESC;
```
## üõ†Ô∏è Services AWS Ma√Ætris√©s
* Amazon S3 : Partitionnement et cycle de vie des donn√©es (Raw vs Analytics).
* AWS Glue : Data Catalog, Crawlers et ETL Jobs (Visual).
* Amazon Athena : Cr√©ation de tables externes et requ√™tes SQL d'analyse.
* IAM : Cr√©ation de Strat√©gies et R√¥les.

